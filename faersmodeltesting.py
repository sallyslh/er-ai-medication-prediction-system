# -*- coding: utf-8 -*-
"""faersmodelTesting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GEIpGhI99EMSBwb9lvYC0-MKX4qpFHVj
"""

from huggingface_hub import login
# Login and mount drive
login()

# Model configuration
hf_model = "epfl-llm/meditron-7b"

"""
COMPREHENSIVE MEDITRON-7B EVALUATION SYSTEM
Evaluates fine-tuned medical AI model for adverse drug reaction prediction
Calculates F1, Precision, Recall, Accuracy, and specialized medical metrics
"""

import torch
import pandas as pd
import numpy as np
import re
import json
from typing import Dict, List, Tuple, Set
from collections import defaultdict, Counter
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    precision_recall_fscore_support, classification_report,
    confusion_matrix
)
from transformers import AutoTokenizer, AutoModelForCausalLM
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

class MedicalAIEvaluator:
    """Comprehensive evaluator for medical AI adverse reaction prediction"""

    def __init__(self, model_path: str, test_data_path: str):
        self.model_path = model_path
        self.test_data_path = test_data_path
        self.model = None
        self.tokenizer = None
        self.test_df = None
        self.predictions = []
        self.parsed_predictions = []
        self.parsed_ground_truth = []

        # Medical evaluation categories
        self.severity_levels = ['severe', 'moderate', 'mild']
        self.common_adverse_reactions = [
            'nausea', 'vomiting', 'diarrhea', 'headache', 'dizziness', 'fatigue',
            'rash', 'itching', 'bleeding', 'liver', 'kidney', 'heart', 'blood',
            'neurological', 'gastrointestinal', 'cardiovascular', 'respiratory'
        ]

    def load_model_and_data(self):
        """Load the fine-tuned model and test data"""
        print("üîÑ Loading fine-tuned Meditron-7b model and test data...")

        # Load model and tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            device_map="auto",
            torch_dtype=torch.float16,
            trust_remote_code=True
        )

        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # Load test data
        self.test_df = pd.read_csv(self.test_data_path)
        print(f"‚úÖ Loaded model and {len(self.test_df)} test examples")

    def generate_predictions(self, max_examples: int = None, batch_size: int = 1):
        """Generate predictions for test examples"""
        if max_examples:
            test_data = self.test_df.head(max_examples)
        else:
            test_data = self.test_df

        print(f"üîÆ Generating predictions for {len(test_data)} examples...")

        predictions = []

        for idx, row in tqdm(test_data.iterrows(), total=len(test_data), desc="Generating predictions"):
            try:
                # Extract input components from the text
                input_text = self.extract_input_from_text(row['text'])

                # Generate prediction
                prediction = self.generate_single_prediction(input_text)

                predictions.append({
                    'index': idx,
                    'input': input_text,
                    'ground_truth': self.extract_output_from_text(row['text']),
                    'prediction': prediction,
                    'original_text': row['text']
                })

            except Exception as e:
                print(f"‚ùå Error generating prediction for index {idx}: {e}")
                predictions.append({
                    'index': idx,
                    'input': '',
                    'ground_truth': '',
                    'prediction': '',
                    'error': str(e)
                })

        self.predictions = predictions
        print(f"‚úÖ Generated {len(predictions)} predictions")

    def extract_input_from_text(self, text: str) -> str:
        """Extract input portion from training text"""
        if "### Input:" in text and "### Output:" in text:
            input_part = text.split("### Input:")[1].split("### Output:")[0].strip()
            return input_part
        return ""

    def extract_output_from_text(self, text: str) -> str:
        """Extract ground truth output from training text"""
        if "### Output:" in text:
            output_part = text.split("### Output:")[-1].strip()
            return output_part
        return ""

    def generate_single_prediction(self, input_text: str) -> str:
        """Generate prediction for a single input"""
        prompt = f"""### Instruction:
You are a clinical pharmacovigilance AI. Predict adverse drug reactions based on patient demographics and medications with their clinical significance levels.

Medication significance:
- Primary Suspect (PS): Most likely causative drug
- Secondary Suspect (SS): Possibly causative drug
- Concomitant (C): Concurrent medication
- Drug Interaction: Potential interaction between drugs

Classify reactions by severity:
- Severe: Life-threatening or serious conditions
- Moderate: Significant symptoms requiring medical attention
- Mild: Minor symptoms or laboratory abnormalities

### Input:
{input_text}

### Output:
"""

        inputs = self.tokenizer(prompt, return_tensors="pt", max_length=1024, truncation=True).to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=200,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.2,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.pad_token_id,
            )

        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Extract only the output portion
        if "### Output:" in response:
            response = response.split("### Output:")[-1].strip()

        return response

    def parse_medical_output(self, text: str) -> Dict:
        """Parse medical output to extract structured information"""
        if not text or pd.isna(text):
            return {
                'severity': 'unknown',
                'reactions': [],
                'has_reactions': False,
                'raw_text': ''
            }

        text_lower = text.lower()

        # Extract severity level
        severity = 'unknown'
        if 'severe:' in text_lower or 'severe ' in text_lower:
            severity = 'severe'
        elif 'moderate:' in text_lower or 'moderate ' in text_lower:
            severity = 'moderate'
        elif 'mild:' in text_lower or 'mild ' in text_lower:
            severity = 'mild'
        elif 'no adverse' in text_lower or 'no reaction' in text_lower:
            severity = 'none'

        # Extract specific reactions mentioned
        reactions = []
        for reaction in self.common_adverse_reactions:
            if reaction in text_lower:
                reactions.append(reaction)

        # Additional reaction extraction using common medical terms
        medical_terms = re.findall(r'\b(?:nausea|vomiting|diarrhea|headache|dizziness|fatigue|rash|bleeding|pain|swelling|fever|cough|shortness|breath)\b', text_lower)
        reactions.extend(medical_terms)

        reactions = list(set(reactions))  # Remove duplicates

        return {
            'severity': severity,
            'reactions': reactions,
            'has_reactions': len(reactions) > 0 or severity != 'none',
            'raw_text': text.strip()
        }

    def parse_all_outputs(self):
        """Parse all predictions and ground truth outputs"""
        print("üîç Parsing predictions and ground truth...")

        self.parsed_predictions = []
        self.parsed_ground_truth = []

        for pred in self.predictions:
            # Parse prediction
            pred_parsed = self.parse_medical_output(pred['prediction'])
            self.parsed_predictions.append(pred_parsed)

            # Parse ground truth
            gt_parsed = self.parse_medical_output(pred['ground_truth'])
            self.parsed_ground_truth.append(gt_parsed)

        print(f"‚úÖ Parsed {len(self.parsed_predictions)} prediction pairs")

    def calculate_severity_metrics(self) -> Dict:
        """Calculate metrics for severity classification"""
        print("üìä Calculating severity classification metrics...")

        pred_severities = [p['severity'] for p in self.parsed_predictions]
        gt_severities = [p['severity'] for p in self.parsed_ground_truth]

        # Create severity label mapping
        severity_labels = ['none', 'mild', 'moderate', 'severe', 'unknown']

        # Convert to numeric labels for sklearn
        pred_numeric = [severity_labels.index(s) if s in severity_labels else 4 for s in pred_severities]
        gt_numeric = [severity_labels.index(s) if s in severity_labels else 4 for s in gt_severities]

        # Calculate metrics
        accuracy = accuracy_score(gt_numeric, pred_numeric)
        precision_macro = precision_score(gt_numeric, pred_numeric, average='macro', zero_division=0)
        recall_macro = recall_score(gt_numeric, pred_numeric, average='macro', zero_division=0)
        f1_macro = f1_score(gt_numeric, pred_numeric, average='macro', zero_division=0)

        # Per-class metrics
        precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(
            gt_numeric, pred_numeric, average=None, zero_division=0
        )

        # Create detailed report
        severity_report = {
            'overall_metrics': {
                'accuracy': accuracy,
                'precision_macro': precision_macro,
                'recall_macro': recall_macro,
                'f1_macro': f1_macro
            },
            'per_class_metrics': {}
        }

        for i, label in enumerate(severity_labels):
            if i < len(precision_per_class):
                severity_report['per_class_metrics'][label] = {
                    'precision': precision_per_class[i],
                    'recall': recall_per_class[i],
                    'f1': f1_per_class[i],
                    'support': int(support[i])
                }

        # Confusion matrix
        cm = confusion_matrix(gt_numeric, pred_numeric)
        severity_report['confusion_matrix'] = cm.tolist()
        severity_report['confusion_matrix_labels'] = severity_labels

        return severity_report

    def calculate_reaction_detection_metrics(self) -> Dict:
        """Calculate metrics for adverse reaction detection (binary classification)"""
        print("üéØ Calculating adverse reaction detection metrics...")

        pred_has_reactions = [p['has_reactions'] for p in self.parsed_predictions]
        gt_has_reactions = [p['has_reactions'] for p in self.parsed_ground_truth]

        # Binary classification metrics
        accuracy = accuracy_score(gt_has_reactions, pred_has_reactions)
        precision = precision_score(gt_has_reactions, pred_has_reactions, zero_division=0)
        recall = recall_score(gt_has_reactions, pred_has_reactions, zero_division=0)
        f1 = f1_score(gt_has_reactions, pred_has_reactions, zero_division=0)

        # True/False Positives/Negatives
        cm = confusion_matrix(gt_has_reactions, pred_has_reactions)

        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'confusion_matrix': cm.tolist(),
            'true_positives': int(cm[1,1]) if cm.shape == (2,2) else 0,
            'false_positives': int(cm[0,1]) if cm.shape == (2,2) else 0,
            'true_negatives': int(cm[0,0]) if cm.shape == (2,2) else 0,
            'false_negatives': int(cm[1,0]) if cm.shape == (2,2) else 0
        }

    def calculate_semantic_similarity_metrics(self) -> Dict:
        """Calculate semantic similarity between predictions and ground truth"""
        print("üî§ Calculating semantic similarity metrics...")

        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity

        pred_texts = [p['raw_text'] for p in self.parsed_predictions]
        gt_texts = [p['raw_text'] for p in self.parsed_ground_truth]

        # Remove empty texts
        valid_pairs = [(p, g) for p, g in zip(pred_texts, gt_texts) if p.strip() and g.strip()]

        if len(valid_pairs) == 0:
            return {'average_similarity': 0.0, 'similarity_scores': []}

        pred_valid, gt_valid = zip(*valid_pairs)

        # Calculate TF-IDF similarity
        all_texts = list(pred_valid) + list(gt_valid)
        vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)

        try:
            tfidf_matrix = vectorizer.fit_transform(all_texts)

            n_pred = len(pred_valid)
            pred_vectors = tfidf_matrix[:n_pred]
            gt_vectors = tfidf_matrix[n_pred:]

            # Calculate pairwise similarities
            similarities = []
            for i in range(len(pred_valid)):
                sim = cosine_similarity(pred_vectors[i], gt_vectors[i])[0][0]
                similarities.append(sim)

            return {
                'average_similarity': np.mean(similarities),
                'similarity_scores': similarities,
                'median_similarity': np.median(similarities),
                'std_similarity': np.std(similarities)
            }

        except Exception as e:
            print(f"‚ùå Error calculating semantic similarity: {e}")
            return {'average_similarity': 0.0, 'similarity_scores': []}

    def calculate_medical_specific_metrics(self) -> Dict:
        """Calculate medical domain-specific metrics"""
        print("üè• Calculating medical domain-specific metrics...")

        metrics = {
            'severity_distribution_accuracy': 0.0,
            'critical_reaction_detection': {},
            'medication_relevance': {}
        }

        # Analyze severity distribution accuracy
        pred_sev_counts = Counter([p['severity'] for p in self.parsed_predictions])
        gt_sev_counts = Counter([p['severity'] for p in self.parsed_ground_truth])

        # Calculate distribution similarity (using KL divergence approximation)
        total_pred = sum(pred_sev_counts.values())
        total_gt = sum(gt_sev_counts.values())

        if total_pred > 0 and total_gt > 0:
            sev_accuracy = 0
            for severity in self.severity_levels:
                pred_prob = pred_sev_counts.get(severity, 0) / total_pred
                gt_prob = gt_sev_counts.get(severity, 0) / total_gt
                sev_accuracy += min(pred_prob, gt_prob)  # Intersection over Union approximation

            metrics['severity_distribution_accuracy'] = sev_accuracy

        # Critical reaction detection (for severe cases)
        severe_indices = [i for i, p in enumerate(self.parsed_ground_truth) if p['severity'] == 'severe']
        if severe_indices:
            severe_detected = sum(1 for i in severe_indices if self.parsed_predictions[i]['severity'] == 'severe')
            metrics['critical_reaction_detection'] = {
                'total_severe_cases': len(severe_indices),
                'correctly_identified_severe': severe_detected,
                'severe_detection_rate': severe_detected / len(severe_indices)
            }

        return metrics

    def generate_comprehensive_report(self) -> Dict:
        """Generate comprehensive evaluation report"""
        print("\nüéØ GENERATING COMPREHENSIVE EVALUATION REPORT")
        print("="*60)

        # Calculate all metrics
        severity_metrics = self.calculate_severity_metrics()
        reaction_metrics = self.calculate_reaction_detection_metrics()
        similarity_metrics = self.calculate_semantic_similarity_metrics()
        medical_metrics = self.calculate_medical_specific_metrics()

        # Compile comprehensive report
        report = {
            'model_info': {
                'model_path': self.model_path,
                'test_data_path': self.test_data_path,
                'total_test_examples': len(self.test_df),
                'evaluated_examples': len(self.predictions)
            },
            'severity_classification': severity_metrics,
            'reaction_detection': reaction_metrics,
            'semantic_similarity': similarity_metrics,
            'medical_specific': medical_metrics,
            'summary_metrics': {
                'overall_accuracy': severity_metrics['overall_metrics']['accuracy'],
                'reaction_detection_f1': reaction_metrics['f1'],
                'semantic_similarity': similarity_metrics.get('average_similarity', 0),
                'critical_detection_rate': medical_metrics.get('critical_reaction_detection', {}).get('severe_detection_rate', 0)
            }
        }

        return report

    def print_evaluation_results(self, report: Dict):
        """Print detailed evaluation results"""
        print("\n" + "="*80)
        print("üè• MEDITRON-7B MODEL EVALUATION RESULTS")
        print("="*80)

        # Model Information
        print(f"\nüìã MODEL INFORMATION:")
        print(f"   Model: {report['model_info']['model_path']}")
        print(f"   Test examples: {report['model_info']['evaluated_examples']:,}")

        # Summary Metrics
        print(f"\nüéØ SUMMARY METRICS:")
        print(f"   Overall Accuracy: {report['summary_metrics']['overall_accuracy']:.4f}")
        print(f"   Reaction Detection F1: {report['summary_metrics']['reaction_detection_f1']:.4f}")
        print(f"   Semantic Similarity: {report['summary_metrics']['semantic_similarity']:.4f}")

        # Severity Classification
        print(f"\n‚öñÔ∏è SEVERITY CLASSIFICATION METRICS:")
        sev_metrics = report['severity_classification']['overall_metrics']
        print(f"   Accuracy: {sev_metrics['accuracy']:.4f}")
        print(f"   Precision (Macro): {sev_metrics['precision_macro']:.4f}")
        print(f"   Recall (Macro): {sev_metrics['recall_macro']:.4f}")
        print(f"   F1 Score (Macro): {sev_metrics['f1_macro']:.4f}")

        # Per-class severity metrics
        print(f"\n   Per-Severity Metrics:")
        for severity, metrics in report['severity_classification']['per_class_metrics'].items():
            if metrics['support'] > 0:
                print(f"     {severity.capitalize():12}: F1={metrics['f1']:.3f}, "
                      f"Precision={metrics['precision']:.3f}, Recall={metrics['recall']:.3f}, "
                      f"Support={metrics['support']}")

        # Adverse Reaction Detection
        print(f"\nüéØ ADVERSE REACTION DETECTION (Binary):")
        reaction_metrics = report['reaction_detection']
        print(f"   Accuracy: {reaction_metrics['accuracy']:.4f}")
        print(f"   Precision: {reaction_metrics['precision']:.4f}")
        print(f"   Recall: {reaction_metrics['recall']:.4f}")
        print(f"   F1 Score: {reaction_metrics['f1']:.4f}")

        # Medical-Specific Metrics
        if 'critical_reaction_detection' in report['medical_specific']:
            crit_metrics = report['medical_specific']['critical_reaction_detection']
            print(f"\nüö® CRITICAL REACTION DETECTION:")
            print(f"   Severe Cases: {crit_metrics['total_severe_cases']}")
            print(f"   Correctly Identified: {crit_metrics['correctly_identified_severe']}")
            print(f"   Detection Rate: {crit_metrics['severe_detection_rate']:.4f}")

        # Performance Assessment
        print(f"\nüí° PERFORMANCE ASSESSMENT:")
        overall_score = (
            report['summary_metrics']['overall_accuracy'] * 0.3 +
            report['summary_metrics']['reaction_detection_f1'] * 0.4 +
            report['summary_metrics']['semantic_similarity'] * 0.3
        )

        if overall_score > 0.8:
            print(f"üèÜ EXCELLENT: Model shows strong performance across all metrics")
        elif overall_score > 0.6:
            print(f"‚úÖ GOOD: Model performs well with room for improvement")
        elif overall_score > 0.4:
            print(f"‚ö†Ô∏è  MODERATE: Model shows basic competency, needs improvement")
        else:
            print(f"‚ùå NEEDS WORK: Model requires significant improvement")

        print(f"   Overall Score: {overall_score:.3f}")

    def create_evaluation_visualizations(self, report: Dict, save_path: str = 'meditron_evaluation.png'):
        """Create comprehensive evaluation visualizations"""
        try:
            fig = plt.figure(figsize=(20, 12))

            # Create subplots
            gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)

            # 1. Severity Classification Confusion Matrix
            ax1 = fig.add_subplot(gs[0, 0])
            cm = np.array(report['severity_classification']['confusion_matrix'])
            labels = report['severity_classification']['confusion_matrix_labels']

            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                       xticklabels=labels, yticklabels=labels, ax=ax1)
            ax1.set_title('Severity Classification\nConfusion Matrix', fontweight='bold')
            ax1.set_xlabel('Predicted Severity')
            ax1.set_ylabel('True Severity')

            # 2. Reaction Detection Metrics
            ax2 = fig.add_subplot(gs[0, 1])
            reaction_metrics = report['reaction_detection']
            metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']
            metrics_values = [reaction_metrics['accuracy'], reaction_metrics['precision'],
                            reaction_metrics['recall'], reaction_metrics['f1']]

            bars = ax2.bar(metrics_names, metrics_values, color=['skyblue', 'lightgreen', 'lightcoral', 'gold'])
            ax2.set_title('Adverse Reaction Detection\nMetrics', fontweight='bold')
            ax2.set_ylabel('Score')
            ax2.set_ylim(0, 1)

            # Add value labels on bars
            for bar, value in zip(bars, metrics_values):
                ax2.annotate(f'{value:.3f}', (bar.get_x() + bar.get_width()/2, bar.get_height()),
                           ha='center', va='bottom', fontweight='bold')

            # 3. Per-Severity F1 Scores
            ax3 = fig.add_subplot(gs[0, 2])
            severity_f1 = []
            severity_names = []

            for severity, metrics in report['severity_classification']['per_class_metrics'].items():
                if metrics['support'] > 0:
                    severity_names.append(severity.capitalize())
                    severity_f1.append(metrics['f1'])

            if severity_names:
                bars = ax3.bar(severity_names, severity_f1, color='lightblue')
                ax3.set_title('F1 Score by Severity Level', fontweight='bold')
                ax3.set_ylabel('F1 Score')
                ax3.set_ylim(0, 1)

                for bar, value in zip(bars, severity_f1):
                    ax3.annotate(f'{value:.3f}', (bar.get_x() + bar.get_width()/2, bar.get_height()),
                               ha='center', va='bottom', fontweight='bold')

            # 4. Overall Performance Radar Chart (simplified)
            ax4 = fig.add_subplot(gs[1, 0])

            summary_metrics = report['summary_metrics']
            performance_aspects = ['Overall\nAccuracy', 'Reaction\nDetection F1', 'Semantic\nSimilarity']
            performance_values = [summary_metrics['overall_accuracy'],
                                summary_metrics['reaction_detection_f1'],
                                summary_metrics['semantic_similarity']]

            bars = ax4.bar(performance_aspects, performance_values, color=['#1f77b4', '#ff7f0e', '#2ca02c'])
            ax4.set_title('Summary Performance Metrics', fontweight='bold')
            ax4.set_ylabel('Score')
            ax4.set_ylim(0, 1)

            for bar, value in zip(bars, performance_values):
                ax4.annotate(f'{value:.3f}', (bar.get_x() + bar.get_width()/2, bar.get_height()),
                           ha='center', va='bottom', fontweight='bold')

            # 5. Model Performance Distribution
            if 'similarity_scores' in report['semantic_similarity'] and report['semantic_similarity']['similarity_scores']:
                ax5 = fig.add_subplot(gs[1, 1])
                similarity_scores = report['semantic_similarity']['similarity_scores']
                ax5.hist(similarity_scores, bins=20, color='lightgreen', alpha=0.7, edgecolor='black')
                ax5.axvline(np.mean(similarity_scores), color='red', linestyle='--',
                           label=f'Mean: {np.mean(similarity_scores):.3f}')
                ax5.set_title('Semantic Similarity Distribution', fontweight='bold')
                ax5.set_xlabel('Similarity Score')
                ax5.set_ylabel('Frequency')
                ax5.legend()

            # 6. Critical Cases Performance
            ax6 = fig.add_subplot(gs[1, 2])
            if 'critical_reaction_detection' in report['medical_specific']:
                crit_data = report['medical_specific']['critical_reaction_detection']
                categories = ['Severe Cases\nDetected', 'Severe Cases\nMissed']
                values = [crit_data['correctly_identified_severe'],
                         crit_data['total_severe_cases'] - crit_data['correctly_identified_severe']]
                colors = ['green', 'red']

                ax6.pie(values, labels=categories, colors=colors, autopct='%1.1f%%', startangle=90)
                ax6.set_title('Critical Case Detection', fontweight='bold')

            plt.suptitle('Meditron-7b Model Evaluation Results', fontsize=16, fontweight='bold')
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.show()

            print(f"üìä Evaluation visualization saved to {save_path}")

        except Exception as e:
            print(f"‚ùå Could not create visualizations: {e}")

    def save_detailed_results(self, report: Dict, save_path: str = 'meditron_evaluation_results.json'):
        """Save detailed evaluation results to JSON"""
        try:
            # Convert numpy types to native Python types for JSON serialization
            def convert_numpy(obj):
                if isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                return obj

            # Add sample predictions for review
            sample_predictions = []
            for i in range(min(10, len(self.predictions))):
                sample_predictions.append({
                    'input': self.predictions[i]['input'],
                    'ground_truth': self.predictions[i]['ground_truth'],
                    'prediction': self.predictions[i]['prediction'],
                    'parsed_gt': self.parsed_ground_truth[i],
                    'parsed_pred': self.parsed_predictions[i]
                })

            report['sample_predictions'] = sample_predictions

            # Clean report for JSON serialization
            clean_report = json.loads(json.dumps(report, default=convert_numpy))

            with open(save_path, 'w') as f:
                json.dump(clean_report, f, indent=2)

            print(f"üíæ Detailed results saved to {save_path}")

        except Exception as e:
            print(f"‚ùå Could not save results: {e}")


def run_meditron_evaluation(model_path: str, test_data_path: str, max_examples: int = None):
    """
    Run comprehensive evaluation of fine-tuned Meditron-7b model

    Args:
        model_path: Path to the fine-tuned model directory
        test_data_path: Path to the test dataset CSV
        max_examples: Maximum number of examples to evaluate (None for all)
    """

    print("\n" + "="*80)
    print("üè• MEDITRON-7B MODEL EVALUATION SYSTEM")
    print("üéØ COMPREHENSIVE MEDICAL AI PERFORMANCE ANALYSIS")
    print("="*80)

    # Initialize evaluator
    evaluator = MedicalAIEvaluator(model_path, test_data_path)

    try:
        # Step 1: Load model and data
        print("\nüìã STEP 1: LOADING MODEL AND TEST DATA")
        print("-" * 50)
        evaluator.load_model_and_data()

        # Step 2: Generate predictions
        print("\nüîÆ STEP 2: GENERATING PREDICTIONS")
        print("-" * 50)
        evaluator.generate_predictions(max_examples=max_examples)

        # Step 3: Parse outputs
        print("\nüîç STEP 3: PARSING MEDICAL OUTPUTS")
        print("-" * 50)
        evaluator.parse_all_outputs()

        # Step 4: Calculate comprehensive metrics
        print("\nüìä STEP 4: CALCULATING COMPREHENSIVE METRICS")
        print("-" * 50)
        report = evaluator.generate_comprehensive_report()

        # Step 5: Display results
        print("\nüìã STEP 5: DISPLAYING EVALUATION RESULTS")
        print("-" * 50)
        evaluator.print_evaluation_results(report)

        # Step 6: Create visualizations
        print("\nüìà STEP 6: CREATING EVALUATION VISUALIZATIONS")
        print("-" * 50)
        evaluator.create_evaluation_visualizations(report)

        # Step 7: Save detailed results
        print("\nüíæ STEP 7: SAVING DETAILED RESULTS")
        print("-" * 50)
        evaluator.save_detailed_results(report)

        print("\n" + "="*80)
        print("üéâ MEDITRON-7B EVALUATION COMPLETED!")
        print("="*80)

        return report

    except Exception as e:
        print(f"‚ùå Evaluation failed: {e}")
        import traceback
        traceback.print_exc()
        return {}


# MAIN EXECUTION - UPDATE THESE PATHS
if __name__ == "__main__":

    # üî• UPDATE THESE PATHS TO YOUR FILES:
    model_path = "/content/drive/MyDrive/meditron7b_faers_efficient/final_model"
    test_data_path = "/content/drive/MyDrive/meditron7b_faers_test_set.csv"

    print("üî¨ Starting Meditron-7b Model Evaluation...")
    print("üéØ This will evaluate F1, Precision, Recall, Accuracy + Medical-specific metrics!")

    # Run evaluation (limit examples for faster testing - remove limit for full evaluation)
    results = run_meditron_evaluation(
        model_path=model_path,
        test_data_path=test_data_path,
        max_examples=100  # Set to None for full evaluation
    )

    if results:
        print(f"\nüéä SUCCESS! Comprehensive evaluation completed!")
        print(f"üìÅ Check these files for detailed results:")
        print(f"   ‚Ä¢ meditron_evaluation_results.json (detailed metrics)")
        print(f"   ‚Ä¢ meditron_evaluation.png (performance charts)")

        # Quick summary
        summary = results.get('summary_metrics', {})
        print(f"\nüéØ QUICK SUMMARY:")
        print(f"   Overall Accuracy: {summary.get('overall_accuracy', 0):.3f}")
        print(f"   Reaction Detection F1: {summary.get('reaction_detection_f1', 0):.3f}")
        print(f"   Semantic Similarity: {summary.get('semantic_similarity', 0):.3f}")
    else:
        print(f"\n‚ùå Evaluation failed. Please check the paths and error messages above.")